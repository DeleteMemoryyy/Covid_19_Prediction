{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "sys.path.append(r'/tf/Lecture/Covid_19_Prediction_Stacking/src/')\n",
    "\n",
    "# import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "\n",
    "from my_dataloader import My_DataLoader\n",
    "from lstm import *\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lr = 0.00002\n",
    "epochs = 1000\n",
    "verbose = True\n",
    "patients = 10\n",
    "verify_every = 5\n",
    "use_cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = My_DataLoader()\n",
    "dl.load_from_csv()\n",
    "country = 'Austria'\n",
    "ori_data = dl.countries_data[country]\n",
    "ori_norm_data = dl.norm_countries_data[country]\n",
    "valid_start = ori_norm_data[ori_norm_data['date']=='11/10/20'].index.values[0]\n",
    "valid_end = ori_norm_data[ori_norm_data['date']=='12/10/20'].index.values[0]\n",
    "ori_norm_data = ori_norm_data.drop(columns=['date'])\n",
    "\n",
    "index_map = {}\n",
    "for i, col in enumerate(ori_data.columns.values):\n",
    "    index_map[col] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_window_train = []\n",
    "for i in range(dl.time_steps, valid_start):\n",
    "    # print('i', i)\n",
    "    new_x = []\n",
    "    for j in range(i - dl.time_steps, i):\n",
    "        # print('j', j)\n",
    "        row = ori_norm_data.loc[j].values.tolist()\n",
    "        new_x.append(row)\n",
    "        \n",
    "    x_window_train.append(new_x)\n",
    "    # print(df.shape)\n",
    "    \n",
    "y_train = ori_data[dl.time_steps : valid_start][['total_cases', 'total_deaths', 'total_recovered', 'total_tests']]\n",
    "x_valid = ori_norm_data[valid_start - dl.time_steps + 1 : valid_start + 1].to_numpy()\n",
    "y_valid = ori_data[valid_start : valid_end][['total_cases', 'total_deaths', 'total_recovered', 'total_tests']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(torch.from_numpy(np.array(x_window_train)).float(), torch.from_numpy(y_train.values.astype('float')).float())\n",
    "train_loader = DataLoader(train_data, shuffle=False, batch_size=dl.batch_size)\n",
    "\n",
    "\n",
    "#\n",
    "# train_data = TensorDataset(torch.from_numpy(x_train.values), torch.from_numpy(y_train.values))\n",
    "# valid_data = TensorDataset(torch.from_numpy(x_valid.values), torch.from_numpy(y_valid.values))\n",
    "# test_data = TensorDataset(torch.from_numpy(data_loader.x_test.values), torch.from_numpy(data_loader.y_test.values))\n",
    "# train_loader = DataLoader(train_data, shuffle=False, batch_size=batch_size, drop_last=True)\n",
    "# valid_loader = DataLoader(valid_data, shuffle=False, batch_size=batch_size, drop_last=True)\n",
    "# test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size, drop_last=True)\n",
    "#\n",
    "# data = data_loader.data\n",
    "# feature_sizes = []\n",
    "# for col in data.columns:\n",
    "#     if col in data_loader.continuous_column:\n",
    "#         feature_sizes.append(1)\n",
    "#     elif col == 'is_trade':\n",
    "#         continue\n",
    "#     else:\n",
    "#         feature_sizes.append(len(data[col].unique()) + 50)\n",
    "#\n",
    "# net = DeepFM(feature_sizes=feature_sizes, continuous_field_size=len(data_loader.continuous_column), num_classes=2,\n",
    "#              use_cuda=use_cuda)\n",
    "# optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=0.03)\n",
    "#\n",
    "# if use_cuda:\n",
    "#     net.cuda()\n",
    "# model = net.fit(train_loader, valid_loader, optimizer, epochs=epochs, verbose=verbose, print_every=print_every)\n",
    "#\n",
    "# test_loss = net.validation(test_loader, model)\n",
    "# print('test_log_loss: {0:f}'.format(test_loss))\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LSTM_Config(feature_size=ori_norm_data.shape[1], time_steps=dl.time_steps)\n",
    "net = LSTM_Model(config)\n",
    "net = net.cuda()\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class My_Train_Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, output, target, info):\n",
    "        print(type(output), type(target), type(info))\n",
    "        return\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "criterion_valid = nn.MSELoss(size_average=True).cuda()\n",
    "criterion_train = My_Train_Loss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "valid_losses = []\n",
    "avg_train_losses = []\n",
    "avg_valid_losses = []\n",
    "for epoch in range(epochs):\n",
    "    net.train()\n",
    "    for batch, (x, y_target) in enumerate(train_loader, 1):\n",
    "        optimizer.zero_grad()\n",
    "        x = x.cuda()\n",
    "        y_target = y_target\n",
    "        y_predict = net(x)\n",
    "        start_index = batch * dl.batch_size + dl.time_steps - 1\n",
    "        loss = criterion_train(y_predict, y_target, ori_data[start_index : start_index + dl.batch_size])\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    if epoch % verify_every == 0:\n",
    "        net.eval()\n",
    "        y_predict = []\n",
    "        # for i in range(valid_start, valid_end):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    loss = criterion_valid(y_predict[:, :3], y_target)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "base",
   "language": "python",
   "display_name": "conda-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}